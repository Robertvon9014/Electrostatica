# Explicaci√≥n de los M√©todos Num√©ricos

## Problema f√≠sico

Se resuelve la ecuaci√≥n de Laplace bidimensional para el potencial el√©ctrico \(\phi(x,y)\):

\[
\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi}{\partial y^2} = 0
\]

Esta ecuaci√≥n nos permite describir el comportamiento de un potencial el√©ctrico sobre una regi√≥n donde no existen cargas libres (es decir, en el 
vac√≠o o en materiales diel√©ctricos).

El dominio de este problema se encuentra  en una placa cuadrada de 10 cm √ó 10 cm, con dos barras internas que simulan las placas de un capacitor:
- Una barra se localiza cerca del borde izquierdo y se mantiene con potencial constante positivo $V_p$.
- Otra barra se encuentra cerca del borde derecho con potencial negativo $V_n$.

La longitud de ambas barras es de 6 cm, y est√°n separadas a la misma distancia que su longitud, lo cual nos permite visualizar una configuraci√≥n sim√©trica con respecto al eje horizontal.

Condiciones del problema:
  - En las condiciones del problemas donde no se especifique el potencial se asume que la condici√≥n de frontera tiene un valor inicial de cero, o 
    bien el potencial se actualiza iterativamente hasta converger.
  
---

## M√©todos implementados

### 1. M√©todo de relajaci√≥n de Jacobi

Este m√©todo itera actualizando el potencial en cada punto de la malla usando el promedio de sus vecinos en la iteraci√≥n anterior, manteniendo fijas las condiciones de frontera.

\[
\phi'(x, y) = \frac{1}{4}\left[\phi(x+a, y) + \phi(x-a, y) + \phi(x, y+a) + \phi(x, y-a)\right],
\]

La idea principal de este m√©todo es que se actualiza el valor del potencial el√©ctrico $\phi(x,y)$ en cada punto de la grilla como el promedio de los valores de sus vecinos adyacentes(arriba, abajo, izquierda, derecha) que se agarran de los datos iterativos anteriores, mientras mantenemos fijas las condiciones de frontera.

Esto se repite hasta que hayamos alcanzado una diferencia m√°xima entre dos iteraciones consecutivas, es lo que llamamos "el delta", y que este sea menor que una tolerancia escogida. Lo que nos indicar√° que hemos alcanzado la convergencia.

- Es sencillo pero convergencia lenta.
- Requiere dos matrices para mantener valores antiguos y nuevos.

---

### 2. M√©todo de sobre-relajaci√≥n de Jacobi (Jacobi modificado)

Se aplica un factor de sobre-relajaci√≥n \(\omega\) para acelerar la convergencia, mezclando el valor nuevo calculado con el anterior:

\[
\phi'(x, y) = (1+\omega)\left[\frac{\phi(x+a, y) + \phi(x-a, y) + \phi(x, y+a) + \phi(x, y-a)}4\right] - \omega \phi (x,y).
\]

- Acelera la convergencia respecto al Jacobi est√°ndar.
- Requiere ajuste del par√°metro \(\omega\) para optimizar rendimiento.

Limitaciones: 
No siempre el m√©todo de Sobre-relajaci√≥n de jacobi funcionar√° ya que este m√©todo es muy inestable para grillas cuadradas, esto sucede debido a que 
el m√©todo no actualiza los valores al instante como lo hace Gauss-Seidel, es por ello que al intentar la sobre-relajaci√≥n los valores del error se 
amplifican, esto debido a las condiciones de frontera internas, como lo son las barras con un voltaje de $\pm 1$, como estan internamente esto 
permite la generaci√≥n de una regi√≥n central en la cual los errores circulan libremente sin ser frenados por los bordes r√≠gidos. Debido a que este 
m√©todo se vuelv num√©ricamente inestable incluso para valores $\omega < 1$, esto nos demuestra que la estabilidad del m√©todo no solo va a depender 
del factor de relajaci√≥n $\omega$, sino tambi√©n de la geometr√≠a del problema y las condiciones de frontera.

Una forma de poder utilizar el m√©todo de sobre-relajaci√≥n de jacobi, para una placa cuadrada con barras internas con voltaje, es utilizar el m√©todo 
de sobre-relajaci√≥n sucesiva (SOR) lo que nos permite esto es hacer iteraciones paralelas sin necesidad de usar valores ya actualizados.

\[
\phi'(x, y) = (1 - \omega) \cdot \phi(x, y) + \frac{\omega}{4} \cdot \left( \phi(x+1, y) + \phi(x-1, y) + \phi(x, y+1) + \phi(x, y-1) \right)
\]

---

### 3. M√©todo de Gauss-Seidel

En este m√©todo, la actualizaci√≥n del potencial en cada punto usa los valores m√°s recientes disponibles (actualizados durante la misma iteraci√≥n), 
mejorando la velocidad de convergencia frente a Jacobi. Esto facilita a la informaci√≥n propagarse m√°s r√°pidamente a trav√©s de la grilla.

\[
\phi(x, y) \leftarrow \frac{\phi(x+a, y) + \phi(x-a, y) + \phi(x, y+a) + \phi(x, y-a)}{4}.
\]

- Solo se necesita una matriz.
- Mejor convergencia en menos iteraciones.
- Los valores reci√©n calculados se utilizan en el mismo ciclo.

aunque este m√©todo es m√°s r√°pido que Jacobi, la forma(orden) de actualizar los puntos puede influir en la estabilidad y el patr√≥n de error.

---

### 4. M√©todo de Gauss-Seidel en C++

Implementaci√≥n equivalente al m√©todo de Gauss-Seidel en Python, pero usando C++ con `std::vector` y `std::tuple`.

- Permite una posible extensi√≥n para paralelizaci√≥n.
- Mejora en eficiencia y manejo de memoria para casos grandes.

---

## 5. M√©todo de Gauss-Seidel en C++ con OpenMP (memoria compartida)

Paralelizamos el m√©todo de Gauss-Seidel utilizando OpenMP y un m√©todo conocido como Red-black ordering, esta t√©cnica nos permite ordenar los puntos
de la grilla como si fuera un tablero de ajedrez (alternando "celdas rojas" y "negras"). Esto facilita que las actualizaciones no presenten conflictos de escritura en memoria compartida.

- paralelismo por hilos usando `#pragma omp parallel`. <br>
- Ordenamiento rojo-negro: actualizamos primero las celas $(i + j)$ par (rojas), luego las impares (negras).<br>

---

## 6. M√©todo de Gauss-Seidel en C++ con MPI (memoria distribuida).

Este paralelismo distribuido utilizado con MPI (Message Passing Interface) a diferencia de OpenMP los datos estan divididos de manera que sus procesos son independientes y se comunican a trav√©s del intercambio de mensajes.

- Se divide la malla de forma horizontal y cada proceso calcula una porci√≥n. <br>
- Se implemente el m√©todo red-black ordering dentro de cada proceso. <br>
- Se realiza una reducci√≥n global del error con `MPI_ALLREDUCE`.

---

## 7. Condiciones de frontera y configuraci√≥n de la grilla

- La placa se discretiza en una malla de \((M+1) \times (M+1)\) puntos.
- Dos barras verticales se colocan a 2 cm de cada borde lateral, con longitudes de 6 cm, y potenciales fijos \(V_p\) y \(V_n\).
- El resto de bordes y posiciones iniciales son 0.

---

## 8. Convergencia y criterio de parada

La iteraci√≥n se detiene cuando la diferencia m√°xima absoluta entre iteraciones consecutivas es menor que una tolerancia dada.

---

## 9. Comparaci√≥n y resultados

- El m√©todo de Gauss-Seidel converge m√°s r√°pido que Jacobi.
- La sobre-relajaci√≥n mejora a√∫n m√°s la velocidad si se escoge un \(\omega\) adecuado (por ejemplo, 0.9). Adem√°s de ello la forma geom√©trica y las condiciones de frontera son las adecuadas.
- La implementaci√≥n en C++ es m√°s eficiente y es base para paralelizaci√≥n futura.

## 10. Gr√°fica de escalabilidad
En esta secci√≥n incluiremos una gr√°fica de escalabilidad tomando en cuenta 2 hilos(threads) y para 3 tama√±os de M distintos(M = 500, 1000, 1500).
![Gr√°fica de escalabilidad](SpeedUp.png)

## 11. An√°lisis
| M    | Hilos | Speedup real | Speedup ideal |
| ---- | ----- | ------------ | ------------- |
| 500  | 1     | 1.00         | 1             |
| 500  | 2     | 1.68         | 2             |
| 1000 | 1     | 1.00         | 1             |
| 1000 | 2     | 1.55         | 2             |
| 1500 | 1     | 1.00         | 1             |
| 1500 | 2     | 1.54         | 2             |

- Speedup ideal es la aceleraci√≥n m√°xima te√≥rica, que aumenta linealmente con el n√∫mero de hilos (por ejemplo, con 2 hilos, el speedup ideal es 2).
- Speedup real muestra los valores obtenidos al correr el c√≥digo en la computadora.

Observaciones
1. Aceleraci√≥n sublineal: <br>
    En todos los casos, el speedup real con 2 hilos est√° por debajo del ideal (1.68, 1.55 y 1.54 en vez de 2). Esto es normal y esperado debido a:

      - Sobrecarga de gesti√≥n de hilos.
      - Costos de sincronizaci√≥n y comunicaci√≥n entre threads.
      - Acceso a memoria compartida y posibles contenciones.

2. Mejor escalabilidad con problema peque√±o (M=500):<br>
    El speedup para $ùëÄ = 500$ con 2 hilos (1.68) es mayor que para $ùëÄ = 1000$ y $ùëÄ = 1500$ (alrededor de 1.55). Esto puede ser por:

      - El trabajo extra por manejar datos mayores y la sobrecarga paralela que no escala igual.
      - O bien, diferencias en la carga de trabajo por iteraci√≥n (aunque usualmente problemas m√°s grandes escalan mejor, puede depender de 
        implementaci√≥n).

3. Escalabilidad limitada a 2 hilos:<br>
      Solo se tiene datos para 1 y 2 hilos. Con solo dos puntos es dif√≠cil trazar conclusiones s√≥lidas. Idealmente, pruebas con m√°s hilos ayudar√≠an 
      a ver tendencias.
